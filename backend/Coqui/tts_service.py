#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Coqui TTS Service
æŒä¹…åŒ–GPUåŠ é€ŸéŸ³é¢‘ç”ŸæˆæœåŠ¡
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import torch
import os
import time
import logging
import uuid
from typing import Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Coqui TTS Service",
    description="é«˜æ€§èƒ½GPUåŠ é€ŸTTSæœåŠ¡",
    version="1.0.0"
)

class AudioRequest(BaseModel):
    text: str
    speaker_wav: Optional[str] = None
    output_filename: Optional[str] = None

class AudioResponse(BaseModel):
    success: bool
    task_id: str
    output_path: str = None
    processing_time: float = None
    error: str = None

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    gpu_available: bool
    gpu_memory_used: float = None

# å…¨å±€TTSæœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹ï¼Œæ¨¡å‹å¸¸é©»ï¼‰
tts_generator = None

class TTSGenerator:
    """TTSç”Ÿæˆå™¨ï¼Œå°è£…æ¨¡å‹åŠ è½½å’ŒéŸ³é¢‘ç”Ÿæˆ"""

    def __init__(self, service_type: str = "coqui"):
        self.service_type = service_type
        # ç±»çº§åˆ«çš„é™æ€ç¼“å­˜ï¼Œæ‰€æœ‰å®ä¾‹å…±äº«
        self._coqui_model = None
        self._coqui_device = None

    def _load_coqui_model(self, device: str = "cuda"):
        """åŠ è½½Coqui TTSæ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # å¦‚æœæ¨¡å‹å·²åŠ è½½ä¸”åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼Œç›´æ¥è¿”å›
        if self._coqui_model is not None and self._coqui_device == device:
            logger.info("âœ… Coqui TTS model already loaded (cached) - skipping loading")
            return self._coqui_model

        try:
            # å°è¯•å¯¼å…¥Coqui TTS
            try:
                from TTS.api import TTS
            except ImportError:
                logger.error("Coqui TTS not installed. Please install with: pip install TTS")
                return None

            # ä¿®å¤ PyTorch 2.6+ çš„ weights_only å®‰å…¨é—®é¢˜
            import torch.serialization
            from TTS.tts.configs.xtts_config import XttsConfig
            from TTS.tts.models.xtts import Xtts, XttsAudioConfig, XttsArgs
            from TTS.config.shared_configs import BaseDatasetConfig
            from TTS.tts.configs.shared_configs import BaseTTSConfig
            torch.serialization.add_safe_globals([
                XttsConfig, Xtts, XttsAudioConfig, BaseDatasetConfig, XttsArgs, BaseTTSConfig
            ])

            # é¦–æ¬¡åŠ è½½æ¨¡å‹
            logger.info("ğŸš€ Loading Coqui TTS model on GPU (first time only)...")
            import time
            start_time = time.time()

            self._coqui_model = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2").to(device)
            self._coqui_device = device

            load_time = time.time() - start_time
            logger.info(f"âœ… Coqui TTS model loaded successfully in {load_time:.1f}s (cached for future use)")

            return self._coqui_model

        except Exception as e:
            logger.error(f"âŒ Error loading Coqui TTS model: {e}")
            return None

    def generate_coqui_audio(self, text: str, output_path: str, speaker_wav: str = None) -> bool:
        """ä½¿ç”¨Coqui TTSç”ŸæˆéŸ³é¢‘ï¼ˆæœ¬åœ°ï¼‰"""
        try:
            # é¦–å…ˆæ£€æŸ¥GPUå¯ç”¨æ€§
            try:
                import torch
                if not torch.cuda.is_available():
                    logger.error("CUDA GPU not available. Coqui TTS requires GPU acceleration.")
                    logger.error("Please ensure:")
                    logger.error("1. NVIDIA GPU is installed")
                    logger.error("2. CUDA drivers are installed")
                    logger.error("3. PyTorch with CUDA support is installed")
                    return False

                # æ£€æŸ¥GPUæ•°é‡å’Œå†…å­˜
                device_count = torch.cuda.device_count()
                current_device = torch.cuda.current_device()
                gpu_name = torch.cuda.get_device_name(current_device)
                total_memory = torch.cuda.get_device_properties(current_device).total_memory / 1024**3  # GB
                allocated_memory = torch.cuda.memory_allocated(current_device) / 1024**3  # GB
                available_memory = total_memory - allocated_memory

                logger.info(f"Using CUDA GPU: {gpu_name}")
                logger.info(f"Total GPU Memory: {total_memory:.1f} GB")
                logger.info(f"Allocated Memory: {allocated_memory:.1f} GB")
                logger.info(f"Available Memory: {available_memory:.1f} GB")
                logger.info(f"Available GPUs: {device_count}")

                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†…å­˜ç”¨äºXTTS
                if available_memory < 3.0:  # XTTSçº¦éœ€è¦2-3GB
                    logger.warning(f"âš ï¸  Low GPU memory available: {available_memory:.1f} GB")
                    logger.warning("XTTS requires ~2-3GB. Model may not cache properly.")
                    logger.warning("Consider stopping other GPU processes for better performance.")

                # è®¾ç½®GPUè®¾å¤‡
                device = "cuda"

            except ImportError:
                logger.error("PyTorch not installed. Please install with: pip install torch")
                return False

            # ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹åŠ è½½
            tts = self._load_coqui_model(device)
            if tts is None:
                logger.error("Failed to load Coqui TTS model")
                return False

            # è®¾ç½®è¯´è¯äººå£°éŸ³æ–‡ä»¶
            speaker_wav_path = speaker_wav or "/home/n8n/AIStudio/default_speaker.wav"

            if os.path.exists(speaker_wav_path):
                logger.info(f"Using speaker voice: {speaker_wav_path}")
                logger.info(f"Generating audio on GPU for text: {text[:50]}...")

                # åœ¨GPUä¸Šæ‰§è¡ŒTTSæ¨ç†
                with torch.cuda.device(device):
                    # æ¸…ç†GPUå†…å­˜ç¼“å­˜
                    torch.cuda.empty_cache()

                    tts.tts_to_file(
                        text=text,
                        speaker_wav=speaker_wav_path,
                        file_path=output_path,
                        language="zh"
                    )

                    logger.info(f"GPU memory used after TTS: {torch.cuda.memory_allocated(device) / 1024**2:.1f} MB")

                    # æ¨ç†å®Œæˆååªæ¸…ç†ä¸´æ—¶ç¼“å­˜ï¼Œä¿ç•™æ¨¡å‹
                    torch.cuda.empty_cache()
                    logger.info("Temporary GPU memory cleaned up, model kept cached")
            else:
                # å¦‚æœæ²¡æœ‰è¯´è¯äººæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è¯­éŸ³
                logger.info("Using default voice")
                logger.info(f"Generating audio on GPU for text: {text[:50]}...")

                # åœ¨GPUä¸Šæ‰§è¡ŒTTSæ¨ç†
                with torch.cuda.device(device):
                    # æ¸…ç†GPUå†…å­˜ç¼“å­˜
                    torch.cuda.empty_cache()

                    tts.tts_to_file(
                        text=text,
                        file_path=output_path,
                        language="zh"
                    )

                    logger.info(f"GPU memory used after TTS: {torch.cuda.memory_allocated(device) / 1024**2:.1f} MB")

                    # æ¨ç†å®Œæˆååªæ¸…ç†ä¸´æ—¶ç¼“å­˜ï¼Œä¿ç•™æ¨¡å‹
                    torch.cuda.empty_cache()
                    logger.info("Temporary GPU memory cleaned up, model kept cached")

            logger.info(f"Audio saved to: {output_path}")

            # ä¿æŒæ¨¡å‹åœ¨GPUä¸Šç¼“å­˜ï¼Œåªæ¸…ç†ä¸´æ—¶å†…å­˜
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("Temporary memory cleanup completed, model kept cached")
            except:
                pass

            return True

        except Exception as e:
            logger.error(f"Error generating Coqui audio: {e}")

            # æä¾›æ›´è¯¦ç»†çš„GPUç›¸å…³é”™è¯¯ä¿¡æ¯
            try:
                import torch
                if torch.cuda.is_available():
                    logger.error(f"GPU is available but TTS failed. GPU info:")
                    logger.error(f"  Device: {torch.cuda.current_device()}")
                    logger.error(f"  Name: {torch.cuda.get_device_name()}")
                    logger.error(f"  Memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
                    logger.error(f"  Memory cached: {torch.cuda.memory_reserved() / 1024**2:.1f} MB")

                    # å°è¯•æ¸…ç†GPUå†…å­˜
                    torch.cuda.empty_cache()
                    logger.error("Attempted to clear GPU memory cache")
                else:
                    logger.error("GPU is not available in the exception handler")
            except ImportError:
                logger.error("PyTorch not available for GPU diagnostics")

            return False

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶åˆå§‹åŒ–æ¨¡å‹"""
    global tts_generator
    logger.info("ğŸš€ Starting Coqui TTS Service...")
    try:
        tts_generator = TTSGenerator(service_type="coqui")
        logger.info("âœ… TTS Service ready!")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize TTS service: {e}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """æœåŠ¡å…³é—­æ—¶æ¸…ç†èµ„æº"""
    global tts_generator
    logger.info("ğŸ›‘ Shutting down TTS Service...")
    # æ¸…ç†GPUå†…å­˜
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except:
        pass
    tts_generator = None
    logger.info("âœ… TTS Service shutdown complete")

@app.post("/generate")
async def generate_audio(request: AudioRequest):
    """ç”ŸæˆéŸ³é¢‘API"""
    global tts_generator

    if not tts_generator:
        raise HTTPException(status_code=503, detail="TTS service not ready")

    task_id = str(uuid.uuid4())
    start_time = time.time()

    try:
        # ç”ŸæˆéŸ³é¢‘åˆ°å†…å­˜ç¼“å†²åŒº
        import io
        from fastapi.responses import StreamingResponse
        
        # ä¸´æ—¶æ–‡ä»¶è·¯å¾„ (TTS åº“å¯èƒ½éœ€è¦æ–‡ä»¶è·¯å¾„)
        # å¦‚æœ TTS åº“æ”¯æŒç›´æ¥å†™å…¥ buffer æœ€å¥½ï¼Œå¦‚æœä¸æ”¯æŒï¼Œå¯èƒ½éœ€è¦å…ˆå†™ä¸´æ—¶æ–‡ä»¶å†è¯»å–
        # Coqui TTS tts_to_file ç¡®å®éœ€è¦æ–‡ä»¶è·¯å¾„
        
        temp_path = f"/tmp/{task_id}.wav"
        
        # ç”ŸæˆéŸ³é¢‘
        success = tts_generator.generate_coqui_audio(
            text=request.text,
            output_path=temp_path,
            speaker_wav=request.speaker_wav
        )

        processing_time = time.time() - start_time

        if success and os.path.exists(temp_path):
            # è¯»å–éŸ³é¢‘æ•°æ®
            with open(temp_path, "rb") as f:
                audio_data = f.read()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_path)
            
            # è¿”å›éŸ³é¢‘æµ
            return StreamingResponse(
                io.BytesIO(audio_data), 
                media_type="audio/wav",
                headers={
                    "X-Task-ID": task_id,
                    "X-Processing-Time": str(processing_time)
                }
            )
        else:
            return AudioResponse(
                success=False,
                task_id=task_id,
                error="Audio generation failed"
            )

    except Exception as e:
        logger.error(f"Audio generation error: {e}")
        return AudioResponse(
            success=False,
            task_id=task_id,
            error=str(e),
            processing_time=time.time() - start_time
        )

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    try:
        import torch

        gpu_available = torch.cuda.is_available()
        model_loaded = tts_generator is not None and tts_generator._coqui_model is not None

        gpu_memory_used = None
        if gpu_available:
            gpu_memory_used = torch.cuda.memory_allocated(0) / 1024**3  # GB

        return HealthResponse(
            status="healthy",
            model_loaded=model_loaded,
            gpu_available=gpu_available,
            gpu_memory_used=gpu_memory_used
        )

    except Exception as e:
        logger.error(f"Health check error: {e}")
        return HealthResponse(
            status="unhealthy",
            model_loaded=False,
            gpu_available=False
        )

@app.get("/stats")
async def get_stats():
    """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
    try:
        import torch
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
            available_memory = total_memory - allocated_memory

            return {
                "gpu_name": torch.cuda.get_device_name(0),
                "total_memory_gb": round(total_memory, 2),
                "allocated_memory_gb": round(allocated_memory, 2),
                "available_memory_gb": round(available_memory, 2),
                "model_loaded": tts_generator is not None and tts_generator._coqui_model is not None
            }
        else:
            return {"error": "GPU not available"}

    except Exception as e:
        return {"error": str(e)}

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "Coqui TTS Service",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "generate": "/generate - POST",
            "health": "/health - GET",
            "stats": "/stats - GET"
        }
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001, workers=1)